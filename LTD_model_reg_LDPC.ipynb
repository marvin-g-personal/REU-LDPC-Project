{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF5M98cVTdxYahKm1rcdHR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/5G-Decoder/blob/main/LTD_model_reg_LDPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5q1VAmIeUKIn",
        "outputId": "30b3770e-9649-4ba6-caca-e0098f9462cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '5G-Decoder'...\n",
            "remote: Enumerating objects: 1352, done.\u001b[K\n",
            "remote: Counting objects: 100% (355/355), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 1352 (delta 239), reused 239 (delta 168), pack-reused 997 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1352/1352), 1.51 MiB | 19.59 MiB/s, done.\n",
            "Resolving deltas: 100% (839/839), done.\n",
            "Requirement already satisfied: sionna in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tensorflow<2.16.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (2.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sionna) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from sionna) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (1.13.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from sionna) (6.4.5)\n",
            "Requirement already satisfied: mitsuba>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (3.5.2)\n",
            "Requirement already satisfied: pythreejs>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from sionna) (2.4.2)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from sionna) (8.0.5)\n",
            "Requirement already satisfied: ipydatawidgets==4.3.2 in /usr/local/lib/python3.10/dist-packages (from sionna) (4.3.2)\n",
            "Requirement already satisfied: jupyterlab-widgets==3.0.5 in /usr/local/lib/python3.10/dist-packages (from sionna) (3.0.5)\n",
            "Requirement already satisfied: traittypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipydatawidgets==4.3.2->sionna) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (4.0.13)\n",
            "Requirement already satisfied: drjit==0.4.6 in /usr/local/lib/python3.10/dist-packages (from mitsuba>=3.2.0->sionna) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0,>=2.13.0->sionna) (2.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16.0,>=2.13.0->sionna) (0.44.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0,>=2.13.0->sionna) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pollyjuice74/5G-Decoder\n",
        "!pip install sionna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "\n",
        "from sionna.fec.utils import generate_reg_ldpc, load_parity_check_examples, LinearEncoder, gm2pcm\n",
        "from sionna.utils.plotting import PlotBER\n",
        "from sionna.fec.ldpc import LDPCBPDecoder\n",
        "\n",
        "import os\n",
        "# os.chdir('../..')\n",
        "if os.path.exists('5G-Decoder'):\n",
        "  os.rename('5G-Decoder', '5G_Decoder')\n",
        "os.chdir('5G_Decoder/adv_nn')\n",
        "\n",
        "from dataset import *\n",
        "from attention import *\n",
        "from channel import *\n",
        "from args import *\n",
        "from model_functs import *\n",
        "from models import *"
      ],
      "metadata": {
        "id": "U5U5qUUVUeRm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading LDPC code\")\n",
        "pcm, k, n, coderate = generate_reg_ldpc(v=3,\n",
        "                                        c=6,\n",
        "                                        n=100,\n",
        "                                        allow_flex_len=True,\n",
        "                                        verbose=True)\n",
        "\n",
        "encoder = LinearEncoder(pcm, is_pcm=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "6RF7dBDwWg0L",
        "outputId": "5bbba962-7289-4451-a960-90f57557ca5b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LDPC code\n",
            "Setting n to:  100\n",
            "Number of edges (VN perspective):  300\n",
            "Number of edges (CN perspective):  300\n",
            "Generated regular (3,6) LDPC code of length n=100\n",
            "Code rate is r=0.500.\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for e2e model\n",
        "from sionna.utils import BinarySource, ebnodb2no\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "# from sionna.fec.ldpc import LDPC5GDecoder, LDPC5GEncoder\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self, model_type, code_type='LDPC', n_look_up=121, k_look_up=80, n=400, k=200,\n",
        "                       n_rings=2, ls_active=True, split_diff=True, sigma=0.1,\n",
        "                       t_layers=1, d_model=128, heads=8, lr=5e-4,\n",
        "                       batch_size=128, batch_size_eval = 150,\n",
        "                       eval_train_iter=10, save_weights_iter=100,\n",
        "                       ebno_db_min=0., ebno_db_max=4., ebno_db_stepsize=0.25,\n",
        "                       traindata_len=500, testdata_len=250, epochs=1000):\n",
        "        assert model_type in ['gen', 'dis'], \"Type must be: 'gen', Generator or 'dis', Discriminator.\"\n",
        "        assert code_type in ['POLAR', 'BCH', 'CCSDS', 'LDPC', 'MACKAY', 'LDPC5G', 'POLAR5G'], \"Invalid linear code type.\"\n",
        "\n",
        "\n",
        "        # model data\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.split_diff = split_diff\n",
        "        self.n_rings = n_rings # ring connectivity of mask\n",
        "        self.sigma = sigma\n",
        "        self.t_layers = t_layers\n",
        "        self.ls_active = ls_active\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "\n",
        "        # training data\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.traindata_len = traindata_len\n",
        "        self.testdata_len = testdata_len\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.ebno_db_min = ebno_db_min\n",
        "        self.ebno_db_max = ebno_db_max\n",
        "        self.ebno_db_stepsize = ebno_db_stepsize\n",
        "        self.eval_train_iter = eval_train_iter\n",
        "        self.save_weights_iter = save_weights_iter\n",
        "        self.batch_size_eval = batch_size_eval\n",
        "\n",
        "        # code data\n",
        "        self.code_type = code_type\n",
        "        self.code = self.get_code(n_look_up, k_look_up) # n,k look up values in Get_Generator_and_Parity\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     self.n, self.m, self.k = self.code.n, self.code.m, self.code.k\n",
        "        # else:\n",
        "        #     self.n, self.m, self.k = n, n-k, k\n",
        "\n",
        "        # self.n_steps = self.m + 5  # Number of diffusion steps\n",
        "\n",
        "    def get_code(self, n_look_up, k_look_up):\n",
        "        code = type('Code', (), {})() # class Code, no base class, no attributes/methods, () instantiate object\n",
        "        # code.n_look_up, code.k_look_up = n_look_up, k_look_up\n",
        "        # code.code_type = self.code_type\n",
        "\n",
        "        # if self.code_type not in ['LDPC5G', 'POLAR5G']:\n",
        "        #     G, H = Get_Generator_and_Parity(code)\n",
        "        #     code.G, code.H = tf.convert_to_tensor(G), csr_matrix( tf.convert_to_tensor(H) )\n",
        "\n",
        "        #     code.m, code.n = code.H.shape\n",
        "        #     code.k = code.n - code.m\n",
        "\n",
        "        return code\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, k, n, return_infobits=False, es_no=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self._n = n\n",
        "        self._k = k\n",
        "\n",
        "        self._binary_source = BinarySource()\n",
        "        self._num_bits_per_symbol = 2\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
        "        self._channel = AWGN()\n",
        "        self._decoder = decoder\n",
        "        self._encoder = encoder\n",
        "        self._return_infobits = return_infobits\n",
        "        self._es_no = es_no\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # no rate-adjustment for uncoded transmission or es_no scenario\n",
        "        if self._decoder is not None and self._es_no==False:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded transmissions the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        if self._encoder is not None:\n",
        "            c = self._encoder(b)\n",
        "        else:\n",
        "            c = b\n",
        "\n",
        "        # check that rate calculations are correct\n",
        "        assert self._n==c.shape[-1], \"Invalid value of n.\"\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        y = self._channel([x, no])\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove zero padded bit at the end\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "\n",
        "        # and run the decoder\n",
        "        if self._decoder is not None:\n",
        "            llr_hat, z_hat, _ = self._decoder(llr)\n",
        "\n",
        "            # z_hat, z_mul, c_t = model.train(x)\n",
        "            # loss = loss_fn(z_hat, z_mul)\n",
        "\n",
        "        if self._return_infobits:\n",
        "            return b, llr_hat\n",
        "        else:\n",
        "            return c, llr_hat\n",
        "\n",
        "\n",
        "class MHAttention(Layer):\n",
        "    def __init__(self, dims, heads, mask_length, linear=True, dropout=0.01):\n",
        "        super().__init__()\n",
        "        assert (dims % heads) == 0, 'dimension must be divisible by the number of heads'\n",
        "        self.linear = linear\n",
        "        self.dims = dims\n",
        "        self.heads = heads\n",
        "        self.dim_head = dims // heads\n",
        "\n",
        "        if linear:\n",
        "            self.k_proj = self.get_k_proj(mask_length) # n+m\n",
        "            self.proj_k = None\n",
        "            self.proj_v = None\n",
        "\n",
        "        self.to_q, self.to_k, self.to_v = [ Dense(self.dims, use_bias=False) for _ in range(3) ]\n",
        "        self.to_out = Dense(dims)\n",
        "        self.dropout = Dropout(dropout) # to d-dimentional embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Creates shape (n,k_proj) proj matrices for key and val in linear attention\n",
        "        n_value = input_shape[1]\n",
        "\n",
        "        if self.linear:\n",
        "            self.proj_k = self.add_weight(\"proj_k\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "            self.proj_v = self.add_weight(\"proj_v\", shape=[n_value, self.k_proj], initializer=GlorotUniform())\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        out_att = self.lin_attention(x, mask) if self.linear else self.attention(x, mask)\n",
        "        return out_att\n",
        "\n",
        "    def get_k_proj(self, mask_length):\n",
        "        # gets dimention for linear tranformer vector projection\n",
        "        for k_proj in range(mask_length // 2, 0, -1): # starts at half the mask length TO 0\n",
        "            if mask_length % k_proj == 0:\n",
        "                return tf.cast(k_proj, tf.int32)\n",
        "\n",
        "    def lin_attention(self, x, mask): # O(n)\n",
        "        shape = tf.shape(x)\n",
        "        b = tf.cast(shape[0], tf.int32)\n",
        "        n = tf.cast(shape[1], tf.int32)\n",
        "\n",
        "        assert x.shape[-1] is not None, \"The last dimension of x is undefined.\"\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n",
        "\n",
        "        # Project key and val into k-dimentional space\n",
        "        key = tf.einsum('bnd,nk->bkd', key, self.proj_k)\n",
        "        val = tf.einsum('bnd,nk->bkd', val, self.proj_v)\n",
        "\n",
        "        # Reshape splitting for heads\n",
        "        query = tf.reshape(query, (b, n, self.heads, self.dim_head))\n",
        "        key = tf.reshape(key, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        val = tf.reshape(val, (b, self.k_proj, self.heads, self.dim_head))\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        # Low-rank mask (n,k_proj)\n",
        "        mask = tf.expand_dims(mask, axis=-1)\n",
        "        mask = tf.image.resize(mask, [n, self.k_proj], method='nearest')\n",
        "        mask = tf.reshape(mask, (1, 1, n, self.k_proj))\n",
        "\n",
        "        # Main attn logic: sftmx( q@k / d**0.5 ) @ v\n",
        "        scores = tf.einsum('bhnd,bhkd->bhnk', query, key) / (tf.sqrt( tf.cast(self.dim_head, dtype=tf.float32) ))\n",
        "        scores += (mask * -1e9) if mask is not None else 0.\n",
        "        attn = tf.nn.softmax(scores, axis=-1) # (b,h,n,k_proj)\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhnk,bhkd->bhnd', attn, val)\n",
        "\n",
        "        # Reshape and pass through out layer\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "    def attention(self, mask): # O(n^2)\n",
        "        shape = tf.shape(x)\n",
        "        b = shape[0]\n",
        "        n = shape[1]\n",
        "\n",
        "        query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n",
        "        query, key, val = [ tf.reshape(x, (b, n, self.heads, self.dim_head)) for x in [query, key, val] ]\n",
        "        query, key, val = [ tf.transpose(x, [0, 2, 1, 3]) for x in [query, key, val] ]\n",
        "\n",
        "        scores = tf.einsum('bhnd,bhnd->bhnn', query, key) / (tf.sqrt(self.dim_head))\n",
        "        scores += (mask * -1e9) if mask is not None else 0. # apply mask non-edge connections\n",
        "        attn = tf.nn.softmax(scores, axis=-1) #-1\n",
        "        attn = self.dropout(attn)\n",
        "        out = tf.einsum('bhnn,bhnm->bhnd', attn, v)\n",
        "\n",
        "        out = tf.transpose(out, [0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (b, n, -1))\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class TransformerDiffusion( Layer ):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.model_type = args.model_type\n",
        "        self.n_steps = args.n_steps\n",
        "\n",
        "        code = args.code\n",
        "        assert isinstance(code.H, tf.sparse.SparseTensor), \"Code's pcm must be sparse.\"\n",
        "        self.pcm = tf.cast(code.H, dtype=tf.int32)\n",
        "        # shapes\n",
        "        self.m, self.n = self.pcm.shape\n",
        "        self.k = self.n - self.m\n",
        "\n",
        "        self.mask = self.create_mask(self.pcm)\n",
        "        # layers\n",
        "        self.src_embed = tf.Variable( tf.random.uniform([1, self.n + self.m, args.d_model]), trainable=True )\n",
        "        self.decoder = Transformer(args.d_model, args.heads, self.mask, args.t_layers)\n",
        "        self.fc = Dense(1)\n",
        "        self.to_n = Dense(1)\n",
        "        self.to_m = Dense(1)\n",
        "        self.time_embed = Embedding(args.n_steps, args.d_model)\n",
        "\n",
        "        self.betas = tf.constant( tf.linspace(1e-3, 1e-2, args.n_steps)*0 + args.sigma )\n",
        "        self.betas_bar = tf.constant( tf.math.cumsum(self.betas, 0) )\n",
        "\n",
        "        self.split_diff = False#args.split_diff\n",
        "        self.ls_active = args.ls_active\n",
        "\n",
        "        scheduler = tf.keras.optimizers.schedules.CosineDecay( initial_learning_rate=args.lr, decay_steps=args.epochs ) # 1000 is size of trainloader\n",
        "        self.optimizer =  tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "\n",
        "    def create_mask(self, H):\n",
        "        m,n = H.shape\n",
        "        mask = tf.eye(n+m, dtype=tf.float32) # (n+m, n+m)\n",
        "        indices = H.indices\n",
        "        cn_con, vn_con = indices[:, 0], indices[:, 1]\n",
        "\n",
        "        for cn, vn_i in zip(cn_con, vn_con):\n",
        "            # cn to vn connections in the mask\n",
        "            mask = tf.tensor_scatter_nd_update(mask, [[n+cn, vn_i],[vn_i, n+cn]], [1.0,1.0])\n",
        "\n",
        "            # distance 2 vn neighbors of vn_i\n",
        "            related_vns = vn_con[cn_con==cn]\n",
        "            for vn_j in related_vns:\n",
        "                mask = tf.tensor_scatter_nd_update(mask, [[vn_i, vn_j],[vn_j, vn_i]], [1.0,1.0])\n",
        "\n",
        "        # -infinity where mask is not set\n",
        "        mask = tf.cast( tf.math.logical_not(mask > 0), dtype=tf.float32) # not(mask > 0) for setting non connections to -1e9\n",
        "        return mask\n",
        "\n",
        "    def get_sigma(self, t):\n",
        "        # make sure t is a positive int\n",
        "        t = tf.cast( tf.abs(t), tf.int32 )\n",
        "        # gather betas\n",
        "        betas_t = tf.gather(self.betas, t)\n",
        "        betas_bar_t = tf.gather(self.betas_bar, t)\n",
        "\n",
        "        return betas_bar_t * betas_t / (betas_bar_t + betas_t)\n",
        "\n",
        "    def get_syndrome(self, r_t):\n",
        "        # Calculate syndrome (pcm @ r = 0) if r is correct in binary\n",
        "        r_t = tf.reshape(r_t, (self.n, -1)) # (n,b)\n",
        "        r_t_bin = tf.cast(llr_to_bin(r_t), dtype=tf.int32)\n",
        "        return tf.sparse.sparse_dense_matmul(self.pcm, r_t_bin) % 2 # (m,n)@(n,b)->(m,b)\n",
        "\n",
        "    # Extracts noise estimate z_hat from r\n",
        "    def tran_call(self, r_t, t):\n",
        "        # Make sure r_t and t are compatible\n",
        "        r_t = tf.reshape(r_t, (self.n, -1)) # (n,b)\n",
        "        t = tf.cast(t, dtype=tf.int32)\n",
        "\n",
        "        # Compute synd and magn\n",
        "        syndrome = tf.reshape( self.get_syndrome(llr_to_bin(r_t)), (self.pcm.shape[0], -1) ) # (m,n)@(n,b)->(m,b) check nodes\n",
        "        magnitude = tf.reshape( tf.abs(r_t), (self.n, -1) ) #(n,b) variable nodes\n",
        "        # make sure their the same dtype\n",
        "        magnitude, syndrome = [ tf.cast(tensor, dtype=tf.float32) for tensor in [magnitude, syndrome] ]\n",
        "\n",
        "        # Concatenate synd and magn\n",
        "        nodes = tf.concat([magnitude, syndrome], axis=0) # data for vertices\n",
        "        nodes = tf.reshape(nodes, (1, self.n+self.m, -1)) # (1, n+m, b)\n",
        "        # print(nodes.shape)\n",
        "\n",
        "        # Embedding nodes w/ attn and 'time' (sum syn errs) dims\n",
        "        nodes_emb = tf.reshape( self.src_embed * nodes, (self.src_embed.shape[-1], self.pcm.shape[0]+self.n, -1) ) # (d,n+m,b)\n",
        "        time_emb = tf.reshape( self.time_embed(t), (self.src_embed.shape[-1], 1, -1) ) # (d,1,b)\n",
        "        tf.print(tf.shape(nodes_emb), tf.shape(time_emb))\n",
        "\n",
        "        # Applying embeds\n",
        "        emb_t = time_emb * nodes_emb # (d, n+m, b)\n",
        "        logits = self.decoder(emb_t) # (d, n+m, d) # TODO: missing batch dims b\n",
        "        # print(emb_t, logits)\n",
        "\n",
        "        # Reduce (d,n+m,d)->(d,n+m)\n",
        "        logits = tf.squeeze( self.fc(logits), axis=-1 )\n",
        "        vn_logits = tf.reshape( logits[:, :self.n], (self.n, -1) ) # (n,d) take the first n logits from the concatenation\n",
        "        cn_logits = tf.reshape( logits[:, self.n:], (self.m, -1) ) # (m,d) take the last m logits from the concatenation\n",
        "        # print(vn_logits, cn_logits)\n",
        "\n",
        "        z_hat = self.to_n(vn_logits)# (n,d)->(n,)\n",
        "        synd = self.to_m(cn_logits)# (m,d)->(m,)\n",
        "        # print(logits.shape, z_hat.shape)\n",
        "\n",
        "        return z_hat, synd\n",
        "\n",
        "    # optimal lambda l for theoretical and for error prediction\n",
        "    def line_search(self, r_t, sigma, err_hat, lin_splits=20):\n",
        "        l_values =  tf.reshape( tf.linspace(1., 20., lin_splits), (1, 1, lin_splits) )\n",
        "        r_t, sigma, err_hat = [ tf.expand_dims(tensor, axis=-1) for tensor in [r_t, sigma, err_hat] ]# (n,b, 1)\n",
        "        # print(f\"sigma: {sigma}, err_hat: {err_hat}\")\n",
        "\n",
        "        # Compute theoretical step size w/ ls splits\n",
        "        z_hat_values = l_values*(sigma*err_hat) # (n,b, l), l is lin_splits\n",
        "        r_values = llr_to_bin(r_t - z_hat_values) # (n,b, l)\n",
        "        r_values = tf.reshape(r_values, [r_values.shape[0], -1]) # (n,b*l)\n",
        "\n",
        "        # sum of synds (m,n)@(n,b*l)->(m,b*l)->(b*l, 1)\n",
        "        sum_synds = tf.reduce_sum( tf.abs( tf.sparse.sparse_dense_matmul(self.pcm, r_values) % 2 ),\n",
        "                                   axis=0 )\n",
        "        sum_synds = tf.reshape(sum_synds, (-1, lin_splits)) # (b, l)\n",
        "        # print(sum_synds.shape)\n",
        "\n",
        "        # Pick optimal ls value\n",
        "        if self.model_type=='dis':\n",
        "             ixs = tf.math.argmin(sum_synds, axis=1, output_type=tf.int32)[:, tf.newaxis] # (b,1) w/ ixs of optimal line search for batch b\n",
        "        elif self.model_type=='gen':\n",
        "             ixs = tf.math.argmax(sum_synds, axis=1, output_type=tf.int32)[:, tf.newaxis] # (b,1)\n",
        "\n",
        "        # print(r_values.shape, z_hat_values.shape, ixs.shape)\n",
        "        # (b, l, n) for indexing on l\n",
        "        r_values, z_hat_values = [ tf.reshape(tensor, [-1, lin_splits, r_values.shape[0]])\n",
        "                                            for tensor in [r_values, z_hat_values] ]\n",
        "\n",
        "        # concat range of batch ixs [0,...,n-1] and optimal line search ixs for gather_nd\n",
        "        indices = tf.concat([ tf.range(ixs.shape[0])[:, tf.newaxis], ixs ],\n",
        "                                                            axis=-1) # (b,2)\n",
        "\n",
        "        # print(r_values, z_hat_values, indices)\n",
        "        # ix on lin_splits w/ gather_nd st. ix,(b, l, n)->(n,b)\n",
        "        r_t1, z_hat = [ tf.reshape( tf.gather_nd(tensor, indices), (self.n, -1) )\n",
        "                                             for tensor in [r_values, z_hat_values] ]\n",
        "        # print(r_t1, z_hat_values)\n",
        "        return r_t1, z_hat # r at t-1\n",
        "\n",
        "    def loss_fn(self, synd):\n",
        "        return tf.reduce_mean(tf.square(synd))\n",
        "\n",
        "    def train_step(self, llr_ch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            _, synd = self.tran_call(llr_ch,\n",
        "                                     tf.reduce_sum( self.get_syndrome(llr_ch) ))\n",
        "            loss = self.loss_fn(synd)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    # def train(self, r_t, struct_noise=0, sim_ampl=True):\n",
        "    #     # t = tf.random.uniform( (c_0.shape[0] // 2 + 1,), minval=0,maxval=self.n_steps, dtype=tf.int32 )\n",
        "    #     # t = tf.concat([t, self.n_steps - t - 1], axis=0)[:c_0.shape[0]] # reshapes t to size x_0\n",
        "    #     # t = tf.cast(t, dtype=tf.int32)\n",
        "\n",
        "    #     # noise_factor = tf.math.sqrt( tf.gather(self.betas_bar, t) )\n",
        "    #     # noise_factor = tf.reshape(noise_factor, (-1, 1))\n",
        "    #     # z = tf.random.normal(c_0.shape)\n",
        "    #     # h = np.random.rayleigh(size=c_0.shape)if sim_ampl else 1.\n",
        "\n",
        "    #     # added noise to codeword\n",
        "    #     # c_t = tf.transpose(h * c_0 + struct_noise + (z*noise_factor))\n",
        "    #     # calculate sum of syndrome\n",
        "    #     t = tf.math.reduce_sum( self.get_syndrome( llr_to_bin(tf.sign(c_t)) ), axis=0 ) # (batch_size, 1)\n",
        "\n",
        "    #     z_hat = self.tran_call(c_t, t) # model prediction\n",
        "\n",
        "    #     if self.model_type=='dis':\n",
        "    #         z_mul = c_t * tf.transpose(c_0) # actual noise added through the channel\n",
        "\n",
        "    #     elif self.model_type=='gen':\n",
        "    #         c_t += z_hat # could contain positive or negative values\n",
        "    #         z_mul = c_t * tf.transpose(c_0) # moidfied channel noise st. it will fool the discriminator\n",
        "\n",
        "    #     z_mul = tf.reshape(z_mul, (z_hat.shape[0], -1))\n",
        "    #     return c_hat, synd #z_hat, llr_to_bin(z_mul), c_t\n",
        "\n",
        "# Construct discriminator (decoder using reverse diffusion)\n",
        "    # Will have to come up with ways to try to decode the noised codeword against specific noise\n",
        "    # that will be trying to fool it.\n",
        "\n",
        "    # For optimization:\n",
        "        # use Linformer having a O(n) on top of already improved complexity using the pcm mask\n",
        "        # use split diffusion to improve accuracy and efficiency by guiding model rather than EMA\n",
        "\n",
        "class Decoder( TransformerDiffusion ):\n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "\n",
        "    # 'test' function\n",
        "    def call(self, r_t):\n",
        "        i = tf.constant(0)  # Initialize loop counter\n",
        "        z_hat = tf.zeros_like(r_t)  # Placeholder for z_hat\n",
        "\n",
        "        def condition(i, r_t, z_hat):\n",
        "            # Loop while i < self.m and syndrome sum is not zero\n",
        "            return tf.logical_and(i < self.m, tf.reduce_sum(self.get_syndrome(r_t)) != 0)\n",
        "\n",
        "        def body(i, r_t, z_hat):\n",
        "            # Perform reverse or split diffusion\n",
        "            r_t, z_hat = tf.cond(\n",
        "                tf.logical_not(self.split_diff),\n",
        "                lambda: self.rev_diff_call(r_t),\n",
        "                lambda: self.split_rdiff_call(r_t),\n",
        "            )\n",
        "            return tf.add(i, 1), r_t, z_hat\n",
        "\n",
        "        # Run tf.while_loop with the loop variables\n",
        "        i, final_r_t, final_z_hat = tf.while_loop(\n",
        "            condition,\n",
        "            body,\n",
        "            loop_vars=[i, r_t, z_hat],\n",
        "            shape_invariants=[i.get_shape(), tf.TensorShape([None, None]), z_hat.get_shape()]\n",
        "        )\n",
        "\n",
        "        return final_r_t, final_z_hat, i\n",
        "\n",
        "\n",
        "    # Refines recieved codeword r at time t\n",
        "    def rev_diff_call(self, r_t):\n",
        "        print(\"Rev def call with line-search...\")\n",
        "        # Make sure r_t and t are compatible\n",
        "        r_t = tf.reshape(r_t, (self.n, -1)) # (n,b)\n",
        "        # 'time step' of diffusion is really ix of abs(sum synd errors)\n",
        "        t = tf.reduce_sum( self.get_syndrome(llr_to_bin(r_t)), axis=0 ) # (m,n)@(n,b)->(m,b)->(1,b)\n",
        "        t = tf.cast(tf.abs(t), dtype=tf.int32)\n",
        "\n",
        "        # Transformer error prediction\n",
        "        z_hat_crude, synd = self.tran_call(r_t, t) # (n,1), (m,1)\n",
        "        # print(\"z_hat_crude: \", z_hat_crude)\n",
        "\n",
        "        # Compute diffusion vars\n",
        "        sigma = self.get_sigma(t) # theoretical step size\n",
        "        # print(\"sigma: \", sigma)\n",
        "        err_hat = r_t - tf.sign(z_hat_crude * r_t) # (n,1)\n",
        "\n",
        "        # Refined estimate of the codeword for the ls diffusion step\n",
        "        r_t1, z_hat = self.line_search(r_t, sigma, err_hat) if self.ls_active else 1.\n",
        "\n",
        "        # Cast both outputs to float32 for consistency\n",
        "        r_t1, z_hat = [ tf.cast(tensor, tf.float32) for tensor in [r_t1, z_hat] ]\n",
        "        # # reshape to (n,b) for consistency\n",
        "        # r_t1, z_hat = [ tf.reshape( tensor, (self.n, -1) )\n",
        "        #                                      for tensor in [r_t1, z_hat] ]\n",
        "\n",
        "        return r_t1, z_hat # r at t-1, both (n,1)\n",
        "\n",
        "    def split_rdiff_call(self, r_t):\n",
        "        print(\"Rev diff call with split diffusion...\")\n",
        "        # Ensure r_t is correctly shaped\n",
        "        r_t = tf.reshape(r_t, (self.n, -1))  # (n,b)\n",
        "        t = tf.reduce_sum(self.get_syndrome(llr_to_bin(r_t)), axis=0)  # (m,n)@(n,b)->(m,b)->(1,b)\n",
        "        t = tf.cast(tf.abs(t), dtype=tf.int32)\n",
        "\n",
        "        # First half-step condition subproblem\n",
        "        z_hat_crude, synd = self.tran_call(r_t, t)\n",
        "        r_t_half = r_t - 0.5 * self.fc(z_hat_crude * self.get_sigma(t))\n",
        "\n",
        "        # Full-step diffusion subproblem\n",
        "        r_t1 = r_t_half + tf.random.normal(r_t_half.shape) * tf.sqrt(self.get_sigma(t))\n",
        "\n",
        "        # Second half-step condition subproblem\n",
        "        z_hat_crude_half, synd = self.tran_call(r_t1, t)  # Reuse the second `tran_call`\n",
        "        r_t1 = r_t1 - 0.5 * self.fc(z_hat_crude_half * self.get_sigma(t))\n",
        "\n",
        "        # Cast both outputs to float32 for consistency\n",
        "        r_t1, z_hat_crude_half = [ tf.cast(tensor, tf.float32) for tensor in [r_t1, z_hat_crude_half] ]\n",
        "        # # reshape to (n,b) for consistency\n",
        "        # r_t1, z_hat_crude_half = [ tf.reshape( tensor, (self.n, -1) )\n",
        "        #                                      for tensor in [r_t1, z_hat_crude_half] ]\n",
        "\n",
        "        return r_t1, z_hat_crude_half  # r at t-1, both (n,1)\n",
        "\n",
        "\n",
        "# args for decoder/discriminator\n",
        "args = Args(model_type='dis')\n",
        "args.code.H = tf.sparse.from_dense(pcm)\n",
        "args.n, args.m = pcm.shape\n",
        "args.k = k\n",
        "args.n_steps = args.m + 5\n",
        "\n",
        "ltd_decoder = Decoder(args) # Linear Transformer Diffusion (LTD) Decoder\n",
        "\n",
        "e2e_ltd = E2EModel(encoder, ltd_decoder, k, n)"
      ],
      "metadata": {
        "id": "XOILyjSGXMdb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dec(model, args):\n",
        "    # loss\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    # optimizer\n",
        "    scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=args.lr, decay_steps=args.epochs) # 1000 is size of trainloader\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler)\n",
        "    # time start\n",
        "    t = time.time()\n",
        "\n",
        "    # SGD update iteration\n",
        "    @tf.function()\n",
        "    def train_step(batch_size):\n",
        "        # train for random SNRs within a pre-defined interval\n",
        "        ebno_db = tf.random.uniform([batch_size, 1],\n",
        "                                    minval=args.ebno_db_min,\n",
        "                                    maxval=args.ebno_db_max)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            c, llr_hat = model(batch_size, ebno_db)\n",
        "            print(c, llr_hat)\n",
        "\n",
        "            loss_value = 0\n",
        "            # we use a multi-loss averaged over all iterations\n",
        "            for _, l in enumerate(llr_hat):\n",
        "                loss_value += loss(c, l)\n",
        "\n",
        "        # and apply the SGD updates\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights) # variables\n",
        "        optimizer.apply_gradients(zip(grads, weights))\n",
        "        return c, llr_hat\n",
        "\n",
        "    print(\"Training Linear Transformer Diffusion Model...\")\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_step(args.batch_size)\n",
        "\n",
        "        # eval train iter\n",
        "        if epoch % args.eval_train_iter == 0:\n",
        "            ebno_db = tf.random.uniform([args.batch_size_eval, 1],\n",
        "                                          minval=args.ebno_db_eval,\n",
        "                                          maxval=args.ebno_db_eval)\n",
        "\n",
        "            c, llr_hat = model(args.batch_size_eval, ebno_db)\n",
        "\n",
        "            loss_value = 0\n",
        "            for l in llr_hat:\n",
        "                loss_value += loss(c, l)\n",
        "\n",
        "            c_hat = tf.cast(tf.greater(llr_hat[-1], 0), tf.float32)\n",
        "            ber = compute_ber(c, c_hat).numpy()\n",
        "\n",
        "            # measure required time since last evaluation\n",
        "            duration = time() - time_start # in s\n",
        "            time_start = time() # reset counter\n",
        "            log_str = f\"Iteration {iter_total}, \" \\\n",
        "                      f\"loss = {loss_value.numpy():.3f}, \" \\\n",
        "                      f\"ber = {ber:.5f}, \" \\\n",
        "                      f\"duration: {duration:.2f}s\"\n",
        "\n",
        "            print(f'Training epoch {epoch}/{args.epochs}, LR={optimizer.learning_rate.numpy():.2e}, Loss={loss.numpy():.5e}, BER={ber}')\n",
        "\n",
        "        # save weights iter\n",
        "        if epoch % args.save_weights_iter == 0:\n",
        "            pass\n",
        "\n",
        "        # heat-map visualization of the model's weights\n",
        "        # for var in self.trainable_variables:\n",
        "        #     var_name = var.name\n",
        "        #     var_value = var.numpy()\n",
        "\n",
        "        #     # Check if the variable is at least 2D (suitable for heatmap)\n",
        "        #     if len(var_value.shape) > 1:\n",
        "        #         plt.figure(figsize=(8, 6))\n",
        "        #         sns.heatmap(var_value, cmap='viridis')\n",
        "        #         plt.title(f'Heatmap of {var_name}')\n",
        "        #         plt.show()\n",
        "        #     else:\n",
        "        #         print(f\"{var_name} has shape {var_value.shape} which is not suitable for a heatmap.\")\n",
        "\n",
        "\n",
        "train_dec(e2e_ltd, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NhZncoEEpgNv",
        "outputId": "4825b000-47f8-4a20-9ab9-30dcbd5ae19a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear Transformer Diffusion Model...\n",
            "Rev def call with line-search...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"<ipython-input-6-cd135426e6cd>\", line 19, in train_step  *\n        c, llr_hat = model(batch_size, ebno_db)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file7k5xc03w.py\", line 100, in tf__call\n        ag__.if_stmt(ag__.ld(self)._decoder is not None, if_body_4, else_body_4, get_state_4, set_state_4, ('llr_hat',), 1)\n    File \"/tmp/__autograph_generated_file7k5xc03w.py\", line 92, in if_body_4\n        (llr_hat, z_hat, _) = ag__.converted_call(ag__.ld(self)._decoder, (ag__.ld(llr),), None, fscope)\n    File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 39, in tf__call\n        (i, final_r_t, final_z_hat) = ag__.converted_call(ag__.ld(tf).while_loop, (ag__.ld(condition), ag__.ld(body)), dict(loop_vars=[ag__.ld(i), ag__.ld(r_t), ag__.ld(z_hat)], shape_invariants=[ag__.converted_call(ag__.ld(i).get_shape, (), None, fscope), ag__.converted_call(ag__.ld(tf).TensorShape, ([None, None],), None, fscope), ag__.converted_call(ag__.ld(z_hat).get_shape, (), None, fscope)]), fscope)\n    File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in body\n        (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n    File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in <lambda>\n        (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n    File \"/tmp/__autograph_generated_file8xofxjca.py\", line 14, in tf__rev_diff_call\n        (z_hat_crude, synd) = ag__.converted_call(ag__.ld(self).tran_call, (ag__.ld(r_t), ag__.ld(t)), None, fscope)\n    File \"/tmp/__autograph_generated_fileqh2vfdrd.py\", line 21, in tf__tran_call\n        logits = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(emb_t),), None, fscope)\n    File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 23, in tf__call\n        ag__.for_stmt(ag__.ld(self).transformer_layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'transformer'})\n    File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 21, in loop_body\n        x = ag__.converted_call(ag__.ld(transformer), (ag__.ld(x), ag__.ld(self).mask), None, fscope)\n    File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n        out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n    File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n        (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n\n    ValueError: Exception encountered when calling layer 'e2e_model_6' (type E2EModel).\n    \n    in user code:\n    \n        File \"<ipython-input-18-798822e143fd>\", line 123, in call  *\n            llr_hat, z_hat, _ = self._decoder(llr)\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 39, in tf__call\n            (i, final_r_t, final_z_hat) = ag__.converted_call(ag__.ld(tf).while_loop, (ag__.ld(condition), ag__.ld(body)), dict(loop_vars=[ag__.ld(i), ag__.ld(r_t), ag__.ld(z_hat)], shape_invariants=[ag__.converted_call(ag__.ld(i).get_shape, (), None, fscope), ag__.converted_call(ag__.ld(tf).TensorShape, ([None, None],), None, fscope), ag__.converted_call(ag__.ld(z_hat).get_shape, (), None, fscope)]), fscope)\n        File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in body\n            (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n        File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in <lambda>\n            (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n        File \"/tmp/__autograph_generated_file8xofxjca.py\", line 14, in tf__rev_diff_call\n            (z_hat_crude, synd) = ag__.converted_call(ag__.ld(self).tran_call, (ag__.ld(r_t), ag__.ld(t)), None, fscope)\n        File \"/tmp/__autograph_generated_fileqh2vfdrd.py\", line 21, in tf__tran_call\n            logits = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(emb_t),), None, fscope)\n        File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 23, in tf__call\n            ag__.for_stmt(ag__.ld(self).transformer_layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'transformer'})\n        File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 21, in loop_body\n            x = ag__.converted_call(ag__.ld(transformer), (ag__.ld(x), ag__.ld(self).mask), None, fscope)\n        File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n            out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n        File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n            out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n        File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n            out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n        File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n            (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n    \n        ValueError: Exception encountered when calling layer 'decoder_6' (type Decoder).\n        \n        in user code:\n        \n            File \"<ipython-input-18-798822e143fd>\", line 429, in body  *\n                r_t, z_hat = tf.cond(\n            File \"<ipython-input-18-798822e143fd>\", line 457, in rev_diff_call  *\n                z_hat_crude, synd = self.tran_call(r_t, t) # (n,1), (m,1)\n            File \"<ipython-input-20-cbba7907387e>\", line 313, in tran_call  *\n                logits = self.decoder(emb_t) # (d, n+m, d) # TODO: missing batch dims b\n            File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 23, in tf__call\n                ag__.for_stmt(ag__.ld(self).transformer_layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'transformer'})\n            File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 21, in loop_body\n                x = ag__.converted_call(ag__.ld(transformer), (ag__.ld(x), ag__.ld(self).mask), None, fscope)\n            File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n                out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n            File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n                out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n            File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n                out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n            File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n                (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n        \n            ValueError: Exception encountered when calling layer 'transformer_6' (type Transformer).\n            \n            in user code:\n            \n                File \"/content/5G_Decoder/adv_nn/transformer.py\", line 29, in call  *\n                    x = transformer(x, self.mask)\n                File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n                    out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n                File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n                    out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n                    out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n                    (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n            \n                ValueError: Exception encountered when calling layer 'transformer_layer_6' (type TransformerLayer).\n                \n                in user code:\n                \n                    File \"/content/5G_Decoder/adv_nn/transformer.py\", line 15, in call  *\n                        out = self.norm1( self.attn(x, mask) )\n                    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n                        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n                        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                    File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n                        (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n                \n                    ValueError: Exception encountered when calling layer 'mh_attention_6' (type MHAttention).\n                    \n                    in user code:\n                    \n                        File \"/content/5G_Decoder/adv_nn/attention.py\", line 68, in call  *\n                            out_att = self.lin_attention(x, mask) if self.linear else self.attention(x, mask)\n                        File \"/content/5G_Decoder/adv_nn/attention.py\", line 82, in lin_attention  *\n                            query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n                        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                            raise e.with_traceback(filtered_tb) from None\n                        File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py\", line 148, in build\n                            raise ValueError(\n                    \n                        ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (128, 150, None)\n                    \n                    \n                    Call arguments received by layer 'mh_attention_6' (type MHAttention):\n                      • x=tf.Tensor(shape=(128, 150, None), dtype=float32)\n                      • mask=tf.Tensor(shape=(150, 150), dtype=float32)\n                \n                \n                Call arguments received by layer 'transformer_layer_6' (type TransformerLayer):\n                  • x=tf.Tensor(shape=(128, 150, None), dtype=float32)\n                  • mask=tf.Tensor(shape=(150, 150), dtype=float32)\n            \n            \n            Call arguments received by layer 'transformer_6' (type Transformer):\n              • x=tf.Tensor(shape=(128, 150, None), dtype=float32)\n        \n        \n        Call arguments received by layer 'decoder_6' (type Decoder):\n          • r_t=tf.Tensor(shape=(128, 100), dtype=float32)\n    \n    \n    Call arguments received by layer 'e2e_model_6' (type E2EModel):\n      • batch_size=tf.Tensor(shape=(), dtype=int32)\n      • ebno_db=tf.Tensor(shape=(128, 1), dtype=float32)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cd135426e6cd>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2e_ltd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-cd135426e6cd>\u001b[0m in \u001b[0;36mtrain_dec\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Linear Transformer Diffusion Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# eval train iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file_cr8yjsu.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mebno_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mebno_db_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mebno_db_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllr_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebno_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllr_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file7k5xc03w.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, batch_size, ebno_db)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mllr_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'llr_hat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'llr_hat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file7k5xc03w.py\u001b[0m in \u001b[0;36mif_body_4\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mif_body_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0;32mnonlocal\u001b[0m \u001b[0mllr_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mllr_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileilbu2xuo.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, r_t)\u001b[0m\n\u001b[1;32m     37\u001b[0m                             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval__2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_r_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_z_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;...\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileilbu2xuo.py\u001b[0m in \u001b[0;36mbody\u001b[0;34m(i, r_t, z_hat)\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mdo_return_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0mretval__2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_diff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrev_diff_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_rdiff_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)...\n\u001b[0m\u001b[1;32m     32\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0mdo_return_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileilbu2xuo.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mdo_return_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0mretval__2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_diff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrev_diff_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_rdiff_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)...\n\u001b[0m\u001b[1;32m     32\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0mdo_return_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file8xofxjca.py\u001b[0m in \u001b[0;36mtf__rev_diff_call\u001b[0;34m(self, r_t)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_syndrome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllr_to_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mz_hat_crude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtran_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0merr_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_hat_crude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileqh2vfdrd.py\u001b[0m in \u001b[0;36mtf__tran_call\u001b[0;34m(self, r_t, t)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0memb_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_emb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mvn_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/5G_Decoder/adv_nn/transformer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'transformer'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/5G_Decoder/adv_nn/transformer.py\u001b[0m in \u001b[0;36mloop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0;32mnonlocal\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'transformer'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/5G_Decoder/adv_nn/transformer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/5G_Decoder/adv_nn/attention.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mout_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'self.linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/5G_Decoder/adv_nn/attention.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mout_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'self.linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/5G_Decoder/adv_nn/attention.py\u001b[0m in \u001b[0;36mtf__lin_attention\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'bnd,nk->bkd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'bnd,nk->bkd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-6-cd135426e6cd>\", line 19, in train_step  *\n        c, llr_hat = model(batch_size, ebno_db)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file7k5xc03w.py\", line 100, in tf__call\n        ag__.if_stmt(ag__.ld(self)._decoder is not None, if_body_4, else_body_4, get_state_4, set_state_4, ('llr_hat',), 1)\n    File \"/tmp/__autograph_generated_file7k5xc03w.py\", line 92, in if_body_4\n        (llr_hat, z_hat, _) = ag__.converted_call(ag__.ld(self)._decoder, (ag__.ld(llr),), None, fscope)\n    File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 39, in tf__call\n        (i, final_r_t, final_z_hat) = ag__.converted_call(ag__.ld(tf).while_loop, (ag__.ld(condition), ag__.ld(body)), dict(loop_vars=[ag__.ld(i), ag__.ld(r_t), ag__.ld(z_hat)], shape_invariants=[ag__.converted_call(ag__.ld(i).get_shape, (), None, fscope), ag__.converted_call(ag__.ld(tf).TensorShape, ([None, None],), None, fscope), ag__.converted_call(ag__.ld(z_hat).get_shape, (), None, fscope)]), fscope)\n    File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in body\n        (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n    File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in <lambda>\n        (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n    File \"/tmp/__autograph_generated_file8xofxjca.py\", line 14, in tf__rev_diff_call\n        (z_hat_crude, synd) = ag__.converted_call(ag__.ld(self).tran_call, (ag__.ld(r_t), ag__.ld(t)), None, fscope)\n    File \"/tmp/__autograph_generated_fileqh2vfdrd.py\", line 21, in tf__tran_call\n        logits = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(emb_t),), None, fscope)\n    File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 23, in tf__call\n        ag__.for_stmt(ag__.ld(self).transformer_layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'transformer'})\n    File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 21, in loop_body\n        x = ag__.converted_call(ag__.ld(transformer), (ag__.ld(x), ag__.ld(self).mask), None, fscope)\n    File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n        out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n    File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n        (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n\n    ValueError: Exception encountered when calling layer 'e2e_model_6' (type E2EModel).\n    \n    in user code:\n    \n        File \"<ipython-input-18-798822e143fd>\", line 123, in call  *\n            llr_hat, z_hat, _ = self._decoder(llr)\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 39, in tf__call\n            (i, final_r_t, final_z_hat) = ag__.converted_call(ag__.ld(tf).while_loop, (ag__.ld(condition), ag__.ld(body)), dict(loop_vars=[ag__.ld(i), ag__.ld(r_t), ag__.ld(z_hat)], shape_invariants=[ag__.converted_call(ag__.ld(i).get_shape, (), None, fscope), ag__.converted_call(ag__.ld(tf).TensorShape, ([None, None],), None, fscope), ag__.converted_call(ag__.ld(z_hat).get_shape, (), None, fscope)]), fscope)\n        File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in body\n            (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n        File \"/tmp/__autograph_generated_fileilbu2xuo.py\", line 31, in <lambda>\n            (r_t, z_hat) = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(tf).logical_not, (ag__.ld(self).split_diff,), None, fscope_2), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).rev_diff_call, (ag__.ld(r_t),), None, fscope_2)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(self).split_rdiff_call, (ag__.ld(r_t),), None, fscope_2))), None, fscope_2)\n        File \"/tmp/__autograph_generated_file8xofxjca.py\", line 14, in tf__rev_diff_call\n            (z_hat_crude, synd) = ag__.converted_call(ag__.ld(self).tran_call, (ag__.ld(r_t), ag__.ld(t)), None, fscope)\n        File \"/tmp/__autograph_generated_fileqh2vfdrd.py\", line 21, in tf__tran_call\n            logits = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(emb_t),), None, fscope)\n        File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 23, in tf__call\n            ag__.for_stmt(ag__.ld(self).transformer_layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'transformer'})\n        File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 21, in loop_body\n            x = ag__.converted_call(ag__.ld(transformer), (ag__.ld(x), ag__.ld(self).mask), None, fscope)\n        File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n            out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n        File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n            out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n        File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n            out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n        File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n            (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n    \n        ValueError: Exception encountered when calling layer 'decoder_6' (type Decoder).\n        \n        in user code:\n        \n            File \"<ipython-input-18-798822e143fd>\", line 429, in body  *\n                r_t, z_hat = tf.cond(\n            File \"<ipython-input-18-798822e143fd>\", line 457, in rev_diff_call  *\n                z_hat_crude, synd = self.tran_call(r_t, t) # (n,1), (m,1)\n            File \"<ipython-input-20-cbba7907387e>\", line 313, in tran_call  *\n                logits = self.decoder(emb_t) # (d, n+m, d) # TODO: missing batch dims b\n            File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 23, in tf__call\n                ag__.for_stmt(ag__.ld(self).transformer_layers, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'transformer'})\n            File \"/tmp/__autograph_generated_filei5_r20f7.py\", line 21, in loop_body\n                x = ag__.converted_call(ag__.ld(transformer), (ag__.ld(x), ag__.ld(self).mask), None, fscope)\n            File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n                out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n            File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n                out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n            File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n                out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n            File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n                (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n        \n            ValueError: Exception encountered when calling layer 'transformer_6' (type Transformer).\n            \n            in user code:\n            \n                File \"/content/5G_Decoder/adv_nn/transformer.py\", line 29, in call  *\n                    x = transformer(x, self.mask)\n                File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_file3f160ddn.py\", line 10, in tf__call\n                    out = ag__.converted_call(ag__.ld(self).norm1, (ag__.converted_call(ag__.ld(self).attn, (ag__.ld(x), ag__.ld(mask)), None, fscope),), None, fscope)\n                File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n                    out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n                    out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n                    (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n            \n                ValueError: Exception encountered when calling layer 'transformer_layer_6' (type TransformerLayer).\n                \n                in user code:\n                \n                    File \"/content/5G_Decoder/adv_nn/transformer.py\", line 15, in call  *\n                        out = self.norm1( self.attn(x, mask) )\n                    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in tf__call\n                        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                    File \"/tmp/__autograph_generated_filezeflzjvj.py\", line 10, in <lambda>\n                        out_att = ag__.if_exp(ag__.ld(self).linear, lambda : ag__.converted_call(ag__.ld(self).lin_attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), lambda : ag__.converted_call(ag__.ld(self).attention, (ag__.ld(x), ag__.ld(mask)), None, fscope), 'self.linear')\n                    File \"/tmp/__autograph_generated_filefc41wdib.py\", line 13, in tf__lin_attention\n                        (query, key, val) = (ag__.converted_call(ag__.ld(self).to_q, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_k, (ag__.ld(x),), None, fscope), ag__.converted_call(ag__.ld(self).to_v, (ag__.ld(x),), None, fscope))\n                \n                    ValueError: Exception encountered when calling layer 'mh_attention_6' (type MHAttention).\n                    \n                    in user code:\n                    \n                        File \"/content/5G_Decoder/adv_nn/attention.py\", line 68, in call  *\n                            out_att = self.lin_attention(x, mask) if self.linear else self.attention(x, mask)\n                        File \"/content/5G_Decoder/adv_nn/attention.py\", line 82, in lin_attention  *\n                            query, key, val = self.to_q(x), self.to_k(x), self.to_v(x)\n                        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                            raise e.with_traceback(filtered_tb) from None\n                        File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py\", line 148, in build\n                            raise ValueError(\n                    \n                        ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (128, 150, None)\n                    \n                    \n                    Call arguments received by layer 'mh_attention_6' (type MHAttention):\n                      • x=tf.Tensor(shape=(128, 150, None), dtype=float32)\n                      • mask=tf.Tensor(shape=(150, 150), dtype=float32)\n                \n                \n                Call arguments received by layer 'transformer_layer_6' (type TransformerLayer):\n                  • x=tf.Tensor(shape=(128, 150, None), dtype=float32)\n                  • mask=tf.Tensor(shape=(150, 150), dtype=float32)\n            \n            \n            Call arguments received by layer 'transformer_6' (type Transformer):\n              • x=tf.Tensor(shape=(128, 150, None), dtype=float32)\n        \n        \n        Call arguments received by layer 'decoder_6' (type Decoder):\n          • r_t=tf.Tensor(shape=(128, 100), dtype=float32)\n    \n    \n    Call arguments received by layer 'e2e_model_6' (type E2EModel):\n      • batch_size=tf.Tensor(shape=(), dtype=int32)\n      • ebno_db=tf.Tensor(shape=(128, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuPmknHQuxFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}