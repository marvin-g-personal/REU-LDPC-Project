{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/REU-LDPC-Project/blob/main/Comp_Diffusion_to_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_k_fbfIf91j"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense\n",
        "\n",
        "!git clone https://github.com/pollyjuice74/gnn-decoder\n",
        "!pip install sionna\n",
        "\n",
        "if os.path.exists('gnn-decoder'):\n",
        "  os.rename('gnn-decoder', 'gnn_decoder')\n",
        "\n",
        "from gnn_decoder.gnn import GNN_BP, UpdateEmbeddings\n",
        "\n",
        "if not os.path.exists('DDECC'):\n",
        "  !git clone https://github.com/pollyjuice74/DDECC\n",
        "os.chdir('DDECC')\n",
        "\n",
        "from Codes import *\n",
        "from DDECC import *\n",
        "from utils import *\n",
        "from args import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The modifications are made so that the model trains on BCH codes of length n=63, k=45, where it says:\n",
        "\n",
        "\"### IMPORTANT ###\""
      ],
      "metadata": {
        "id": "pGW0jSfZQ5vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The modifications made are seen in the **FEC_Dataset** where it says:\n",
        "\n",
        "\"### IMPORTANT ###\""
      ],
      "metadata": {
        "id": "cc9SRvk2QS3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QXuKlYv-n6Vj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FEC_Dataset(data.Dataset):                 ####\n",
        "    def __init__(self, code, sigma, len, zero_cw=True):\n",
        "        self.code = code\n",
        "        self.sigma = sigma\n",
        "        self.len = len\n",
        "        self.generator_matrix = code.generator_matrix.transpose(0, 1)\n",
        "        self.pc_matrix = code.pc_matrix.transpose(0, 1)\n",
        "\n",
        "        self.zero_word = torch.zeros((self.code.k)).long() if zero_cw else None\n",
        "        self.zero_cw = torch.zeros((self.code.n)).long() if zero_cw else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.zero_cw is None:\n",
        "            m = torch.randint(0, 2, (1, self.code.k)).squeeze()\n",
        "            x = torch.matmul(m, self.generator_matrix) % 2\n",
        "        else: # SET TO TRUE\n",
        "            m = self.zero_word\n",
        "            x = self.zero_cw\n",
        "\n",
        "        #h = torch.from_numpy(np.random.rayleigh(1,self.code.n)).float()\n",
        "        # y = x.clone()\n",
        "\n",
        "        # # Random bit flipping error\n",
        "        # ix = torch.tensor(random.sample(range(self.code.n), 3))\n",
        "        # y[ix] = 1 - y[ix] # flip bits\n",
        "        # y = bin_to_sign(y)\n",
        "\n",
        "        ### IMPORTANT ###\n",
        "        #######################################################################\n",
        "        # Make noise\n",
        "        std_noise = random.choice(self.sigma)\n",
        "        z = torch.randn(self.code.n) * std_noise\n",
        "        # Convert y to sign and add noise\n",
        "        h=1\n",
        "        y = h*bin_to_sign(x) + z\n",
        "\n",
        "        # Sign to LLR conversion\n",
        "        var = std_noise ** 2\n",
        "        def sign_to_llr(bpsk_vect, noise_variance):\n",
        "          return 2*bpsk_vect / noise_variance\n",
        "\n",
        "        # x,y to llrs\n",
        "        x = bin_to_sign(x)\n",
        "        x_llr = sign_to_llr(x, var)\n",
        "        y_llr = sign_to_llr(y, var)\n",
        "        #######################################################################\n",
        "\n",
        "        magnitude = torch.abs(y)\n",
        "        syndrome = torch.matmul(sign_to_bin(torch.sign(y)).long(),\n",
        "                                self.pc_matrix) % 2\n",
        "        syndrome = bin_to_sign(syndrome)\n",
        "        return m.float(), x.float(), z.float(), y.float(), x_llr.float(), y_llr.float(), magnitude.float(), syndrome.float()\n",
        "\n",
        "\n",
        "### DIFFUSION FUNCTIONS ###\n",
        "\n",
        "def train_dif(model, device, train_loader, optimizer, epoch, LR):\n",
        "    model.train()\n",
        "    cum_loss = cum_samples = 0\n",
        "    t = time.time()\n",
        "    for batch_idx, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(train_loader):\n",
        "        # stop at batch 25\n",
        "        if batch_idx==25:\n",
        "          break\n",
        "\n",
        "        loss = model.loss(bin_to_sign(x))\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.ema.update(model)\n",
        "\n",
        "        cum_loss += loss.item() * x.shape[0]\n",
        "        cum_samples += x.shape[0]\n",
        "        if (batch_idx+1) % 25 == 0 or batch_idx == len(train_loader) - 1:\n",
        "            print(f'Training epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}: LR={LR:.2e}, Loss={cum_loss / cum_samples:.5e}')\n",
        "            break\n",
        "    print(f'Epoch {epoch} Train Time {time.time() - t}s\\n')\n",
        "    return cum_loss / cum_samples\n",
        "\n",
        "\n",
        "def test_dif(model, device, test_loader_list, EbNo_range_test, min_FER=100, max_cum_count=1e7, min_cum_count=1e5):\n",
        "    model.eval()\n",
        "    test_loss_ber_list, test_loss_fer_list, cum_samples_all = [], [], []\n",
        "    t = time.time()\n",
        "    with torch.no_grad():\n",
        "        for ii, test_loader in enumerate(test_loader_list): # just the first three\n",
        "            # stop at batch 5\n",
        "            if ii==5:\n",
        "              break\n",
        "\n",
        "            test_ber = test_fer = cum_count = 0.\n",
        "            _, x_pred_list, _, _ = model.p_sample_loop(next(iter(test_loader))[3])\n",
        "            test_ber_ddpm , test_fer_ddpm = [0]*len(x_pred_list), [0]*len(x_pred_list)\n",
        "            idx_conv_all = []\n",
        "            printed = False # Flag for printing x, x_pred\n",
        "\n",
        "\n",
        "            for batch_ix, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(test_loader):\n",
        "                # stop at batch 5\n",
        "                if batch_ix==5:\n",
        "                  break\n",
        "\n",
        "                x_pred, x_pred_list, idx_conv,synd_all = model.p_sample_loop(y)\n",
        "                # convert to binary\n",
        "                x_pred = sign_to_bin(torch.sign(x_pred))\n",
        "                x = sign_to_bin(x)\n",
        "\n",
        "                idx_conv_all.append(idx_conv)\n",
        "                for kk, x_pred_tmp in enumerate(x_pred_list):\n",
        "                    x_pred_tmp = sign_to_bin(torch.sign(x_pred_tmp))\n",
        "\n",
        "                    test_ber_ddpm[kk] += BER(x_pred_tmp, x) * x.shape[0]\n",
        "                    test_fer_ddpm[kk] += FER(x_pred_tmp, x) * x.shape[0]\n",
        "\n",
        "                test_ber += BER(x_pred, x) * x.shape[0]\n",
        "                test_fer += FER(x_pred, x) * x.shape[0]\n",
        "                cum_count += x.shape[0]\n",
        "\n",
        "                if not printed:\n",
        "                  print(\"x: \", x)\n",
        "                  print(\"x_pred: \", x_pred)\n",
        "                  printed = True\n",
        "\n",
        "                break # from while loop\n",
        "\n",
        "            idx_conv_all = torch.stack(idx_conv_all).float()\n",
        "            cum_samples_all.append(cum_count)\n",
        "            test_loss_ber_list.append(test_ber / cum_count)\n",
        "            test_loss_fer_list.append(test_fer / cum_count)\n",
        "            for kk in range(len(test_ber_ddpm)):\n",
        "                test_ber_ddpm[kk] /= cum_count\n",
        "                test_fer_ddpm[kk] /= cum_count\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, BER={test_loss_ber_list}')\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, BER_DDPM={test_ber_ddpm}')\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, -ln(BER)_DDPM={[-np.log(elem) for elem in test_ber_ddpm]}')\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, FER_DDPM={test_fer_ddpm}')\n",
        "            print(f'#It. to zero syndrome: Mean={idx_conv_all.mean()}, Std={idx_conv_all.std()}, Min={idx_conv_all.min()}, Max={idx_conv_all.max()}')\n",
        "\n",
        "        print('Test FER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_fer_list, EbNo_range_test))]))\n",
        "        print('Test BER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "        print('Test -ln(BER) ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, -np.log(elem)) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "    print(f'# of testing samples: {cum_samples_all}\\n Test Time {time.time() - t} s\\n')\n",
        "    return test_loss_ber_list, test_loss_fer_list\n",
        "\n",
        "\n",
        "### GNN FUNCTIONS ###\n",
        "\n",
        "def train_gnn(model, device, train_loader, optimizer, epoch, LR):\n",
        "    # model.train()\n",
        "    cum_loss = cum_samples = 0\n",
        "    t = time.time()\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    for batch_idx, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(train_loader): # train_loader size 1000\n",
        "        # stop at batch 250\n",
        "        if batch_idx==250:\n",
        "          break\n",
        "        # convert to tf for GNN_BP\n",
        "        y_llr = tf.convert_to_tensor(y_llr.numpy(), dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "          # model prediction\n",
        "          x_llr_hat = model(y_llr)\n",
        "\n",
        "          loss = loss_fn(x_llr, x_llr_hat)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        cum_loss += loss * x.shape[0]\n",
        "        cum_samples += x.shape[0]\n",
        "        if (batch_idx+1) % 5 == 0 or batch_idx == len(train_loader) - 1:\n",
        "            print(f'Training epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}: LR={LR:.2e}, Loss={cum_loss / cum_samples:.5e}')\n",
        "\n",
        "    print(f'Epoch {epoch} Train Time {time.time() - t}s\\n')\n",
        "    return cum_loss / cum_samples\n",
        "\n",
        "\n",
        "def test_gnn(model, device, test_loader_list, EbNo_range_test, min_FER=100, max_cum_count=1e7, min_cum_count=1e5):\n",
        "    # model.eval()\n",
        "    test_loss_ber_list, test_loss_fer_list, cum_samples_all = [], [], []\n",
        "    t = time.time()\n",
        "    printed = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ii, test_loader in enumerate(test_loader_list):\n",
        "            # stop at batch 5\n",
        "            if ii==5:\n",
        "              break\n",
        "\n",
        "            test_ber = test_fer = cum_count = 0.\n",
        "            # idx_conv_all = []\n",
        "            # Iterate over first 5 batches\n",
        "            for batch_ix, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(test_loader):\n",
        "                # stop at batch 5\n",
        "                if batch_ix==5:\n",
        "                  break\n",
        "\n",
        "                # convert llr from pytorch to tensorflow\n",
        "                y_llr = tf.convert_to_tensor(y_llr.numpy(), dtype=tf.float32)\n",
        "\n",
        "                # Model prediction\n",
        "                x_llr_hat = model(y_llr)\n",
        "                x_hat = torch.tensor(x_llr_hat.numpy() > 0, dtype=torch.float32)\n",
        "\n",
        "                # Convert x from bpsk to binary\n",
        "                x = sign_to_bin(x)\n",
        "\n",
        "                # FER, BER\n",
        "                test_ber += BER(x_hat, x) * x.shape[0]\n",
        "                test_fer += FER(x_hat, x) * x.shape[0]\n",
        "                cum_count += x.shape[0]\n",
        "\n",
        "                # Show prediction and actual x\n",
        "                if not printed:\n",
        "                  print(\"GNN x_hat: \", x_hat)\n",
        "                  print(\"Actual x: \", x)\n",
        "                  printed = True\n",
        "\n",
        "                if (min_FER > 0 and test_fer > min_FER and cum_count > min_cum_count) or cum_count >= max_cum_count:\n",
        "                    if cum_count >= 1e9:\n",
        "                        print(f'Cum count reached EbN0:{EbNo_range_test[ii]}')\n",
        "                    else:\n",
        "                        print(f'FER count treshold reached EbN0:{EbNo_range_test[ii]}')\n",
        "                    break\n",
        "\n",
        "\n",
        "            # idx_conv_all = torch.stack(idx_conv_all).float()\n",
        "            cum_samples_all.append(cum_count)\n",
        "            test_loss_ber_list.append(test_ber / cum_count)\n",
        "            test_loss_fer_list.append(test_fer / cum_count)\n",
        "\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, BER={test_loss_ber_list}')\n",
        "            # print(f'#It. to zero syndrome: Mean={idx_conv_all.mean()}, Std={idx_conv_all.std()}, Min={idx_conv_all.min()}, Max={idx_conv_all.max()}')\n",
        "\n",
        "        print('Test FER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_fer_list, EbNo_range_test))]))\n",
        "        print('Test BER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "        print('Test -ln(BER) ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, -np.log(elem)) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "    print(f'# of testing samples: {cum_samples_all}\\n Test Time {time.time() - t} s\\n')\n",
        "    return test_loss_ber_list, test_loss_fer_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4VhjwNEljG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1507532-0e12-4d22-e326-54f045e0152f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to model/logs: DDECCT_Results/LDPC__Code_n_121_k_80__26_06_2024_21_42_02\n",
            "Diffusion # of Parameters: 52503\n",
            "GNN # of Parameters: 0\n",
            "\n",
            "Creating data...\n",
            "Training model with code type: LDPC\n",
            "\n",
            "\n",
            "Training DDECCT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1, Batch 25/1000: LR=5.00e-04, Loss=4.59557e-01\n",
            "Epoch 1 Train Time 58.65085005760193s\n",
            "\n",
            "Testing DDECCT...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "args = pass_args_ddecc()\n",
        "\n",
        "code = args.code\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Models\n",
        "ddecct = DDECCT(args, device=device,dropout=0).to(device)\n",
        "ddecct.ema.register(ddecct)\n",
        "gnn = GNN_BP(code)\n",
        "\n",
        "# Optimizers and schedulers\n",
        "optimizer_dif = torch.optim.Adam(ddecct.parameters(), lr=args.lr)\n",
        "scheduler_dif = CosineAnnealingLR(optimizer_dif, T_max=args.epochs, eta_min=5e-6)\n",
        "print(f'Diffusion # of Parameters: {np.sum([np.prod(p.shape) for p in ddecct.parameters()])}')\n",
        "\n",
        "scheduler_gnn = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=args.lr, decay_steps=args.epochs*1000) # 1000 is size of trainloader\n",
        "optimizer_gnn = tf.keras.optimizers.Adam(learning_rate=scheduler_gnn)\n",
        "print(f'GNN # of Parameters: {len(gnn.trainable_variables)}\\n')\n",
        "\n",
        "print(\"Creating data...\")\n",
        "EbNo_range_test = range(4, 7)\n",
        "EbNo_range_train = range(2, 8)\n",
        "std_train = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_train]\n",
        "std_test = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_test]\n",
        "train_dataloader = DataLoader(FEC_Dataset(code, std_train, len=args.batch_size * 1000, zero_cw=True), batch_size=int(args.batch_size),\n",
        "                              shuffle=True, num_workers=args.workers)\n",
        "test_dataloader_list = [DataLoader(FEC_Dataset(code, [std_test[ii]], len=int(args.test_batch_size), zero_cw=False),\n",
        "                                    batch_size=int(args.test_batch_size), shuffle=False, num_workers=args.workers) for ii in range(len(std_test))]\n",
        "\n",
        "print(f\"Training model with code type: {args.code_type}\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Train for 25 data loader batches\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    print(\"Training GNN...\")\n",
        "    train_gnn(gnn, device, train_dataloader,\n",
        "              optimizer_gnn, epoch, LR=scheduler_gnn(tf.Variable(0, dtype=tf.float32)).numpy())\n",
        "    print(\"Training DDECCT...\")\n",
        "    train_dif(ddecct, device, train_dataloader,\n",
        "              optimizer_dif, epoch, LR=scheduler_dif.get_last_lr()[0])\n",
        "\n",
        "    # print comparison\n",
        "    if epoch % 1 == 0 or epoch in [1,5]:\n",
        "        print(\"Testing GNN...\")\n",
        "        test_gnn(gnn, device, test_dataloader_list,\n",
        "                 EbNo_range_test,min_FER=50,max_cum_count=1e6,min_cum_count=1e4)\n",
        "        print(\"Testing DDECCT...\")\n",
        "        test_dif(ddecct, device, test_dataloader_list,\n",
        "                 EbNo_range_test,min_FER=50,max_cum_count=1e6,min_cum_count=1e4)\n",
        "    break # from for loop\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMRXH9GFMikPMKv7PJUNL2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}