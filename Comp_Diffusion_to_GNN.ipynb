{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/REU-LDPC-Project/blob/main/Comp_Diffusion_to_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This Notebook compares the decoding performance over a zero codeword on LDPC codes of n=121, k=66, m=55 of two decoders.\n",
        "\n",
        "- GNN: Graph Neural Network implementing **Belief Propagation**.\n",
        "- DDECCT: Denoising Diffusion Error Correcting Code Transformer implementing **Diffusion and Transformers**.\n",
        "\n",
        "Source code imported: [GNN](https://github.com/pollyjuice74/gnn-decoder), [DDECCT](https://github.com/pollyjuice74/DDECC)."
      ],
      "metadata": {
        "id": "LG2ijq220_kW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_k_fbfIf91j"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense\n",
        "\n",
        "!git clone https://github.com/pollyjuice74/gnn-decoder\n",
        "!pip install sionna\n",
        "\n",
        "if os.path.exists('gnn-decoder'):\n",
        "  os.rename('gnn-decoder', 'gnn_decoder')\n",
        "\n",
        "from gnn_decoder.gnn import GNN_BP, UpdateEmbeddings\n",
        "\n",
        "if not os.path.exists('DDECC'):\n",
        "  !git clone https://github.com/pollyjuice74/DDECC\n",
        "os.chdir('DDECC')\n",
        "\n",
        "from Codes import *\n",
        "from DDECC import *\n",
        "from utils import *\n",
        "from args import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell defines the Dataset used and the respective train/test functions for GNN and DDECC models."
      ],
      "metadata": {
        "id": "qTkwoxrUzi95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QXuKlYv-n6Vj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FEC_Dataset(data.Dataset):                 ####\n",
        "    def __init__(self, code, sigma, len, zero_cw=True):\n",
        "        self.code = code\n",
        "        self.sigma = sigma\n",
        "        self.len = len\n",
        "        self.generator_matrix = code.generator_matrix.transpose(0, 1)\n",
        "        self.pc_matrix = code.pc_matrix.transpose(0, 1)\n",
        "\n",
        "        self.zero_word = torch.zeros((self.code.k)).long() if zero_cw else None\n",
        "        self.zero_cw = torch.zeros((self.code.n)).long() if zero_cw else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.zero_cw is None:\n",
        "            m = torch.randint(0, 2, (1, self.code.k)).squeeze()\n",
        "            x = torch.matmul(m, self.generator_matrix) % 2\n",
        "        else: # SET TO TRUE\n",
        "            m = self.zero_word\n",
        "            x = self.zero_cw\n",
        "\n",
        "        #h = torch.from_numpy(np.random.rayleigh(1,self.code.n)).float()\n",
        "        # y = x.clone()\n",
        "\n",
        "        # # Random bit flipping error\n",
        "        # ix = torch.tensor(random.sample(range(self.code.n), 3))\n",
        "        # y[ix] = 1 - y[ix] # flip bits\n",
        "        # y = bin_to_sign(y)\n",
        "\n",
        "        ### IMPORTANT ###\n",
        "        #######################################################################\n",
        "        # Make noise\n",
        "        std_noise = random.choice(self.sigma)\n",
        "        z = torch.randn(self.code.n) * std_noise\n",
        "        # Convert y to sign and add noise\n",
        "        h=1\n",
        "        y = h*bin_to_sign(x) + z\n",
        "\n",
        "        # Sign to LLR conversion\n",
        "        var = std_noise ** 2\n",
        "        def sign_to_llr(bpsk_vect, noise_variance):\n",
        "          return 2*bpsk_vect / noise_variance\n",
        "\n",
        "        # x,y to llrs\n",
        "        x = bin_to_sign(x)\n",
        "        x_llr = sign_to_llr(x, var)\n",
        "        y_llr = sign_to_llr(y, var)\n",
        "        #######################################################################\n",
        "\n",
        "        magnitude = torch.abs(y)\n",
        "        syndrome = torch.matmul(sign_to_bin(torch.sign(y)).long(),\n",
        "                                self.pc_matrix) % 2\n",
        "        syndrome = bin_to_sign(syndrome)\n",
        "        return m.float(), x.float(), z.float(), y.float(), x_llr.float(), y_llr.float(), magnitude.float(), syndrome.float()\n",
        "\n",
        "\n",
        "### DIFFUSION FUNCTIONS ###\n",
        "\n",
        "def train_dif(model, device, train_loader, optimizer, epoch, LR):\n",
        "    model.train()\n",
        "    cum_loss = cum_samples = 0\n",
        "    t = time.time()\n",
        "    for batch_idx, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(train_loader):\n",
        "        # stop at batch 25\n",
        "        if batch_idx==25:\n",
        "          break\n",
        "\n",
        "        loss = model.loss(bin_to_sign(x))\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.ema.update(model)\n",
        "\n",
        "        cum_loss += loss.item() * x.shape[0]\n",
        "        cum_samples += x.shape[0]\n",
        "        if (batch_idx+1) % 25 == 0 or batch_idx == len(train_loader) - 1:\n",
        "            print(f'Training epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}: LR={LR:.2e}, Loss={cum_loss / cum_samples:.5e}')\n",
        "            break\n",
        "    print(f'Epoch {epoch} Train Time {time.time() - t}s\\n')\n",
        "    return cum_loss / cum_samples\n",
        "\n",
        "\n",
        "def test_dif(model, device, test_loader_list, EbNo_range_test, min_FER=100, max_cum_count=1e7, min_cum_count=1e5):\n",
        "    model.eval()\n",
        "    test_loss_ber_list, test_loss_fer_list, cum_samples_all = [], [], []\n",
        "    t = time.time()\n",
        "    printed = False # Flag for printing x, x_pred\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ii, test_loader in enumerate(test_loader_list): # just the first three\n",
        "            # stop at batch 5\n",
        "            if ii==5:\n",
        "              break\n",
        "            test_ber = test_fer = cum_count = 0.\n",
        "\n",
        "            for batch_ix, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(test_loader):\n",
        "\n",
        "                x_pred, x_pred_list, idx_conv,synd_all = model.p_sample_loop(y)\n",
        "\n",
        "                # convert to binary\n",
        "                x_pred = sign_to_bin(torch.sign(x_pred))\n",
        "                x = sign_to_bin(x)\n",
        "\n",
        "                test_ber += BER(x_pred, x) * x.shape[0]\n",
        "                test_fer += FER(x_pred, x) * x.shape[0]\n",
        "                cum_count += x.shape[0]\n",
        "\n",
        "                if not printed:\n",
        "                  print(\"x: \", x)\n",
        "                  print(\"x_pred: \", x_pred)\n",
        "                  printed = True\n",
        "\n",
        "                break # from while loop\n",
        "\n",
        "            cum_samples_all.append(cum_count)\n",
        "            test_loss_ber_list.append(test_ber / cum_count)\n",
        "            test_loss_fer_list.append(test_fer / cum_count)\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, BER={test_loss_ber_list}')\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, FER={test_loss_fer_list}')\n",
        "\n",
        "        print('Test FER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_fer_list, EbNo_range_test))]))\n",
        "        print('Test BER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "        print('Test -ln(BER) ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, -np.log(elem)) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "    print(f'# of testing samples: {cum_samples_all}\\n Test Time {time.time() - t} s\\n')\n",
        "    return test_loss_ber_list, test_loss_fer_list\n",
        "\n",
        "\n",
        "### GNN FUNCTIONS ###\n",
        "\n",
        "def train_gnn(model, device, train_loader, optimizer, epoch, LR):\n",
        "    # model.train()\n",
        "    cum_loss = cum_samples = 0\n",
        "    t = time.time()\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    for batch_idx, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(train_loader): # train_loader size 1000\n",
        "        # stop at batch 250\n",
        "        if batch_idx==250:\n",
        "          break\n",
        "        # convert to tf for GNN_BP\n",
        "        y_llr = tf.convert_to_tensor(y_llr.numpy(), dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "          # model prediction\n",
        "          x_llr_hat = model(y_llr)\n",
        "\n",
        "          loss = loss_fn(x_llr, x_llr_hat)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        cum_loss += loss * x.shape[0]\n",
        "        cum_samples += x.shape[0]\n",
        "        if (batch_idx+1) % 5 == 0 or batch_idx == len(train_loader) - 1:\n",
        "            print(f'Training epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}: LR={LR:.2e}, Loss={cum_loss / cum_samples:.5e}')\n",
        "\n",
        "    print(f'Epoch {epoch} Train Time {time.time() - t}s\\n')\n",
        "    return cum_loss / cum_samples\n",
        "\n",
        "\n",
        "def test_gnn(model, device, test_loader_list, EbNo_range_test, min_FER=100, max_cum_count=1e7, min_cum_count=1e5):\n",
        "    # model.eval()\n",
        "    test_loss_ber_list, test_loss_fer_list, cum_samples_all = [], [], []\n",
        "    t = time.time()\n",
        "    printed = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ii, test_loader in enumerate(test_loader_list):\n",
        "            # stop at batch 5\n",
        "            if ii==5:\n",
        "              break\n",
        "\n",
        "            test_ber = test_fer = cum_count = 0.\n",
        "            # idx_conv_all = []\n",
        "            # Iterate over first 5 batches\n",
        "            for batch_ix, (m, x, z, y, x_llr, y_llr, magnitude, syndrome) in enumerate(test_loader):\n",
        "                # stop at batch 5\n",
        "                if batch_ix==5:\n",
        "                  break\n",
        "\n",
        "                # convert llr from pytorch to tensorflow\n",
        "                y_llr = tf.convert_to_tensor(y_llr.numpy(), dtype=tf.float32)\n",
        "\n",
        "                # Model prediction\n",
        "                x_llr_hat = model(y_llr)\n",
        "                x_hat = torch.tensor(x_llr_hat.numpy() > 0, dtype=torch.float32)\n",
        "\n",
        "                # Convert x from bpsk to binary\n",
        "                x = sign_to_bin(x)\n",
        "\n",
        "                # FER, BER\n",
        "                test_ber += BER(x_hat, x) * x.shape[0]\n",
        "                test_fer += FER(x_hat, x) * x.shape[0]\n",
        "                cum_count += x.shape[0]\n",
        "\n",
        "                # Show prediction and actual x\n",
        "                if not printed:\n",
        "                  print(\"GNN x_hat: \", x_hat)\n",
        "                  print(\"Actual x: \", x)\n",
        "                  printed = True\n",
        "\n",
        "                if (min_FER > 0 and test_fer > min_FER and cum_count > min_cum_count) or cum_count >= max_cum_count:\n",
        "                    if cum_count >= 1e9:\n",
        "                        print(f'Cum count reached EbN0:{EbNo_range_test[ii]}')\n",
        "                    else:\n",
        "                        print(f'FER count treshold reached EbN0:{EbNo_range_test[ii]}')\n",
        "                    break\n",
        "\n",
        "\n",
        "            # idx_conv_all = torch.stack(idx_conv_all).float()\n",
        "            cum_samples_all.append(cum_count)\n",
        "            test_loss_ber_list.append(test_ber / cum_count)\n",
        "            test_loss_fer_list.append(test_fer / cum_count)\n",
        "\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, BER={test_loss_ber_list}')\n",
        "            # print(f'#It. to zero syndrome: Mean={idx_conv_all.mean()}, Std={idx_conv_all.std()}, Min={idx_conv_all.min()}, Max={idx_conv_all.max()}')\n",
        "\n",
        "        print('Test FER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_fer_list, EbNo_range_test))]))\n",
        "        print('Test BER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "        print('Test -ln(BER) ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, -np.log(elem)) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "    print(f'# of testing samples: {cum_samples_all}\\n Test Time {time.time() - t} s\\n')\n",
        "    return test_loss_ber_list, test_loss_fer_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell runs the comparison for the GNN and DDECCT models.\n",
        "\n",
        "### Training:\n",
        "- GNN: Trains on **250 batches** (around 5 minutes).\n",
        "- DDECCT: Trains on **25 batches** (less than a minute).\n",
        "\n",
        "### Interpreting results:\n",
        "- GNN: Decodes with a BER of around **0.18**.\n",
        "- DDECCT: Decodes with BER of around **0.05**.\n"
      ],
      "metadata": {
        "id": "LcjFFzl3zV2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f4VhjwNEljG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e28fe7-9079-4dff-9e06-ad750f7583da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to model/logs: DDECCT_Results/LDPC__Code_n_121_k_80__26_06_2024_22_54_08\n",
            "Diffusion # of Parameters: 52503\n",
            "GNN # of Parameters: 0\n",
            "\n",
            "Creating data...\n",
            "Training model with code type: LDPC\n",
            "\n",
            "\n",
            "Training GNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1, Batch 5/1000: LR=5.00e-04, Loss=-4.04266e+00\n",
            "Training epoch 1, Batch 10/1000: LR=5.00e-04, Loss=-6.75766e+00\n",
            "Training epoch 1, Batch 15/1000: LR=5.00e-04, Loss=-9.71730e+00\n",
            "Training epoch 1, Batch 20/1000: LR=5.00e-04, Loss=-1.26450e+01\n",
            "Training epoch 1, Batch 25/1000: LR=5.00e-04, Loss=-1.56792e+01\n",
            "Training epoch 1, Batch 30/1000: LR=5.00e-04, Loss=-1.86511e+01\n",
            "Training epoch 1, Batch 35/1000: LR=5.00e-04, Loss=-2.16774e+01\n",
            "Training epoch 1, Batch 40/1000: LR=5.00e-04, Loss=-2.47491e+01\n",
            "Training epoch 1, Batch 45/1000: LR=5.00e-04, Loss=-2.79905e+01\n",
            "Training epoch 1, Batch 50/1000: LR=5.00e-04, Loss=-3.11454e+01\n",
            "Training epoch 1, Batch 55/1000: LR=5.00e-04, Loss=-3.43118e+01\n",
            "Training epoch 1, Batch 60/1000: LR=5.00e-04, Loss=-3.73516e+01\n",
            "Training epoch 1, Batch 65/1000: LR=5.00e-04, Loss=-4.04182e+01\n",
            "Training epoch 1, Batch 70/1000: LR=5.00e-04, Loss=-4.31935e+01\n",
            "Training epoch 1, Batch 75/1000: LR=5.00e-04, Loss=-4.61640e+01\n",
            "Training epoch 1, Batch 80/1000: LR=5.00e-04, Loss=-4.90408e+01\n",
            "Training epoch 1, Batch 85/1000: LR=5.00e-04, Loss=-5.19410e+01\n",
            "Training epoch 1, Batch 90/1000: LR=5.00e-04, Loss=-5.49244e+01\n",
            "Training epoch 1, Batch 95/1000: LR=5.00e-04, Loss=-5.78742e+01\n",
            "Training epoch 1, Batch 100/1000: LR=5.00e-04, Loss=-6.08808e+01\n",
            "Training epoch 1, Batch 105/1000: LR=5.00e-04, Loss=-6.37152e+01\n",
            "Training epoch 1, Batch 110/1000: LR=5.00e-04, Loss=-6.68681e+01\n",
            "Training epoch 1, Batch 115/1000: LR=5.00e-04, Loss=-6.98462e+01\n",
            "Training epoch 1, Batch 120/1000: LR=5.00e-04, Loss=-7.29295e+01\n",
            "Training epoch 1, Batch 125/1000: LR=5.00e-04, Loss=-7.59443e+01\n",
            "Training epoch 1, Batch 130/1000: LR=5.00e-04, Loss=-7.90150e+01\n",
            "Training epoch 1, Batch 135/1000: LR=5.00e-04, Loss=-8.21258e+01\n",
            "Training epoch 1, Batch 140/1000: LR=5.00e-04, Loss=-8.53759e+01\n",
            "Training epoch 1, Batch 145/1000: LR=5.00e-04, Loss=-8.87298e+01\n",
            "Training epoch 1, Batch 150/1000: LR=5.00e-04, Loss=-9.18848e+01\n",
            "Training epoch 1, Batch 155/1000: LR=5.00e-04, Loss=-9.51410e+01\n",
            "Training epoch 1, Batch 160/1000: LR=5.00e-04, Loss=-9.85643e+01\n",
            "Training epoch 1, Batch 165/1000: LR=5.00e-04, Loss=-1.01870e+02\n",
            "Training epoch 1, Batch 170/1000: LR=5.00e-04, Loss=-1.05402e+02\n",
            "Training epoch 1, Batch 175/1000: LR=5.00e-04, Loss=-1.08732e+02\n",
            "Training epoch 1, Batch 180/1000: LR=5.00e-04, Loss=-1.12160e+02\n",
            "Training epoch 1, Batch 185/1000: LR=5.00e-04, Loss=-1.15522e+02\n",
            "Training epoch 1, Batch 190/1000: LR=5.00e-04, Loss=-1.19285e+02\n",
            "Training epoch 1, Batch 195/1000: LR=5.00e-04, Loss=-1.23057e+02\n",
            "Training epoch 1, Batch 200/1000: LR=5.00e-04, Loss=-1.26721e+02\n",
            "Training epoch 1, Batch 205/1000: LR=5.00e-04, Loss=-1.30604e+02\n",
            "Training epoch 1, Batch 210/1000: LR=5.00e-04, Loss=-1.34383e+02\n",
            "Training epoch 1, Batch 215/1000: LR=5.00e-04, Loss=-1.38326e+02\n",
            "Training epoch 1, Batch 220/1000: LR=5.00e-04, Loss=-1.42159e+02\n",
            "Training epoch 1, Batch 225/1000: LR=5.00e-04, Loss=-1.46058e+02\n",
            "Training epoch 1, Batch 230/1000: LR=5.00e-04, Loss=-1.49360e+02\n",
            "Training epoch 1, Batch 235/1000: LR=5.00e-04, Loss=-1.49543e+02\n",
            "Training epoch 1, Batch 240/1000: LR=5.00e-04, Loss=-1.52179e+02\n",
            "Training epoch 1, Batch 245/1000: LR=5.00e-04, Loss=-1.55244e+02\n",
            "Training epoch 1, Batch 250/1000: LR=5.00e-04, Loss=-1.58817e+02\n",
            "Epoch 1 Train Time 279.1313488483429s\n",
            "\n",
            "Training DDECCT...\n",
            "Training epoch 1, Batch 25/1000: LR=5.00e-04, Loss=4.62558e-01\n",
            "Epoch 1 Train Time 10.90805983543396s\n",
            "\n",
            "Testing GNN...\n",
            "GNN x_hat:  tensor([[1., 0., 1.,  ..., 0., 1., 1.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
            "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
            "        [1., 1., 0.,  ..., 1., 0., 1.]])\n",
            "Actual x:  tensor([[1., 0., 0.,  ..., 1., 1., 1.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
            "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
            "        [0., 0., 0.,  ..., 1., 0., 1.]])\n",
            "Test EbN0=4, BER=[0.18904151022434235]\n",
            "Test EbN0=5, BER=[0.18904151022434235, 0.15483358502388]\n",
            "Test EbN0=6, BER=[0.18904151022434235, 0.15483358502388, 0.12444715201854706]\n",
            "Test FER 4: 1.00e+00 5: 1.00e+00 6: 1.00e+00\n",
            "Test BER 4: 1.89e-01 5: 1.55e-01 6: 1.24e-01\n",
            "Test -ln(BER) 4: 1.67e+00 5: 1.87e+00 6: 2.08e+00\n",
            "# of testing samples: [2048.0, 2048.0, 2048.0]\n",
            " Test Time 19.497345447540283 s\n",
            "\n",
            "Testing DDECCT...\n",
            "x:  tensor([[0., 1., 1.,  ..., 1., 1., 0.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
            "        [1., 0., 1.,  ..., 0., 1., 0.]])\n",
            "x_pred:  tensor([[0., 1., 1.,  ..., 1., 1., 0.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
            "        ...,\n",
            "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 0., 1.,  ..., 0., 1., 1.]])\n",
            "Test EbN0=4, BER=[0.04994592443108559]\n",
            "Test EbN0=4, FER=[0.98388671875]\n",
            "Test EbN0=5, BER=[0.04994592443108559, 0.035846300423145294]\n",
            "Test EbN0=5, FER=[0.98388671875, 0.90478515625]\n",
            "Test EbN0=6, BER=[0.04994592443108559, 0.035846300423145294, 0.02374822460114956]\n",
            "Test EbN0=6, FER=[0.98388671875, 0.90478515625, 0.72265625]\n",
            "Test FER 4: 9.84e-01 5: 9.05e-01 6: 7.23e-01\n",
            "Test BER 4: 4.99e-02 5: 3.58e-02 6: 2.37e-02\n",
            "Test -ln(BER) 4: 3.00e+00 5: 3.33e+00 6: 3.74e+00\n",
            "# of testing samples: [2048.0, 2048.0, 2048.0]\n",
            " Test Time 335.0817360877991 s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "args = pass_args_ddecc()\n",
        "\n",
        "code = args.code\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Models\n",
        "ddecct = DDECCT(args, device=device,dropout=0).to(device)\n",
        "ddecct.ema.register(ddecct)\n",
        "gnn = GNN_BP(code)\n",
        "\n",
        "# Optimizers and schedulers\n",
        "optimizer_dif = torch.optim.Adam(ddecct.parameters(), lr=args.lr)\n",
        "scheduler_dif = CosineAnnealingLR(optimizer_dif, T_max=args.epochs, eta_min=5e-6)\n",
        "print(f'Diffusion # of Parameters: {np.sum([np.prod(p.shape) for p in ddecct.parameters()])}')\n",
        "\n",
        "scheduler_gnn = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=args.lr, decay_steps=args.epochs*1000) # 1000 is size of trainloader\n",
        "optimizer_gnn = tf.keras.optimizers.Adam(learning_rate=scheduler_gnn)\n",
        "print(f'GNN # of Parameters: {len(gnn.trainable_variables)}\\n')\n",
        "\n",
        "print(\"Creating data...\")\n",
        "EbNo_range_test = range(4, 7)\n",
        "EbNo_range_train = range(2, 8)\n",
        "std_train = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_train]\n",
        "std_test = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_test]\n",
        "train_dataloader = DataLoader(FEC_Dataset(code, std_train, len=args.batch_size * 1000, zero_cw=True), batch_size=int(args.batch_size),\n",
        "                              shuffle=True, num_workers=args.workers)\n",
        "test_dataloader_list = [DataLoader(FEC_Dataset(code, [std_test[ii]], len=int(args.test_batch_size), zero_cw=False),\n",
        "                                    batch_size=int(args.test_batch_size), shuffle=False, num_workers=args.workers) for ii in range(len(std_test))]\n",
        "\n",
        "print(f\"Training model with code type: {args.code_type}\\n\\n\")\n",
        "\n",
        "\n",
        "# Train for 25 data loader batches\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    print(\"Training GNN...\")\n",
        "    train_gnn(gnn, device, train_dataloader,\n",
        "              optimizer_gnn, epoch, LR=scheduler_gnn(tf.Variable(0, dtype=tf.float32)).numpy())\n",
        "    print(\"Training DDECCT...\")\n",
        "    train_dif(ddecct, device, train_dataloader,\n",
        "              optimizer_dif, epoch, LR=scheduler_dif.get_last_lr()[0])\n",
        "\n",
        "    # print comparison\n",
        "    if epoch % 1 == 0 or epoch in [1,5]:\n",
        "        print(\"Testing GNN...\")\n",
        "        test_gnn(gnn, device, test_dataloader_list,\n",
        "                 EbNo_range_test,min_FER=50,max_cum_count=1e6,min_cum_count=1e4)\n",
        "        print(\"Testing DDECCT...\")\n",
        "        test_dif(ddecct, device, test_dataloader_list,\n",
        "                 EbNo_range_test,min_FER=50,max_cum_count=1e6,min_cum_count=1e4)\n",
        "    break # from for loop\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}